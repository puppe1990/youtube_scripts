{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "# Suppress urllib3 OpenSSL warning (common on macOS with LibreSSL)\n",
    "# This warning appears when urllib3 v2 is used with LibreSSL instead of OpenSSL\n",
    "# It's safe to ignore as it doesn't affect functionality\n",
    "warnings.filterwarnings('ignore', message='.*urllib3.*OpenSSL.*')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='urllib3')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yt_dlp  # More up-to-date alternative to youtube_dl\n",
    "\n",
    "def normalize_channel_url(url):\n",
    "    \"\"\"\n",
    "    Normalizes different YouTube channel URL formats to a standard format.\n",
    "    Handles:\n",
    "    - https://www.youtube.com/channel/CHANNEL_ID\n",
    "    - https://www.youtube.com/@username\n",
    "    - https://www.youtube.com/user/USERNAME\n",
    "    - https://www.youtube.com/c/CHANNEL_NAME\n",
    "    Returns the normalized URL pointing to the channel's videos page.\n",
    "    \"\"\"\n",
    "    url = url.strip()\n",
    "    \n",
    "    # If already a channel ID URL with /videos, return as is\n",
    "    if '/channel/' in url and '/videos' in url:\n",
    "        return url\n",
    "    \n",
    "    # If it's a channel ID URL without /videos, add it\n",
    "    channel_id_match = re.search(r'/channel/([^/?]+)', url)\n",
    "    if channel_id_match:\n",
    "        channel_id = channel_id_match.group(1)\n",
    "        return f\"https://www.youtube.com/channel/{channel_id}/videos\"\n",
    "    \n",
    "    # If it's an @username URL, yt-dlp can handle it directly\n",
    "    if '/@' in url:\n",
    "        # Remove trailing slash and add /videos if not present\n",
    "        url = url.rstrip('/')\n",
    "        if not url.endswith('/videos'):\n",
    "            return f\"{url}/videos\"\n",
    "        return url\n",
    "    \n",
    "    # For other formats, try to extract channel ID by crawling\n",
    "    # But first, let yt-dlp try to handle it directly\n",
    "    return url\n",
    "\n",
    "def get_video_ids_and_channel_name(channel_url):\n",
    "    \"\"\"\n",
    "    Uses yt-dlp to extract video IDs, URLs and the channel name from the YouTube channel.\n",
    "    Handles various URL formats including @username, channel ID, etc.\n",
    "    Returns a tuple: (channel_name, list_of_video_ids, list_of_video_urls)\n",
    "    \"\"\"\n",
    "    # Normalize the URL\n",
    "    normalized_url = normalize_channel_url(channel_url)\n",
    "    \n",
    "    # Ensure we're pointing to the videos page\n",
    "    if not normalized_url.endswith('/videos'):\n",
    "        if '/@' in normalized_url:\n",
    "            normalized_url = f\"{normalized_url.rstrip('/')}/videos\"\n",
    "        elif '/channel/' in normalized_url:\n",
    "            normalized_url = f\"{normalized_url.rstrip('/')}/videos\"\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'extract_flat': True,      # Only fetch basic info (no download)\n",
    "        'skip_download': True,     # Do not download videos\n",
    "        'quiet': False,            # Set to False to see progress\n",
    "        'ignoreerrors': True,\n",
    "        'playlistend': None,       # Get all videos\n",
    "    }\n",
    "    video_ids = []\n",
    "    video_urls = []\n",
    "    channel_name = None\n",
    "    \n",
    "    print(f\"Fetching channel info from: {normalized_url}\")\n",
    "    \n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            info = ydl.extract_info(normalized_url, download=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting channel info: {e}\")\n",
    "            # Try alternative approach - extract channel ID first\n",
    "            try:\n",
    "                response = requests.get(normalized_url, timeout=10)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                for link in soup.head.find_all(\"link\"):\n",
    "                    href = link.get(\"href\", \"\")\n",
    "                    match = re.search(r'/channel/([^/?]+)', href)\n",
    "                    if match:\n",
    "                        channel_id = match.group(1)\n",
    "                        alt_url = f\"https://www.youtube.com/channel/{channel_id}/videos\"\n",
    "                        print(f\"Trying alternative URL: {alt_url}\")\n",
    "                        info = ydl.extract_info(alt_url, download=False)\n",
    "                        break\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e2:\n",
    "                print(f\"Alternative method also failed: {e2}\")\n",
    "                raise ValueError(f\"Could not extract channel information: {e}\")\n",
    "        \n",
    "        if not info:\n",
    "            raise ValueError(\"No channel information found\")\n",
    "        \n",
    "        # Extract channel name\n",
    "        channel_name = info.get(\"channel\", info.get(\"uploader\", info.get(\"title\", \"Channel\")))\n",
    "        if not channel_name or channel_name == \"Channel\":\n",
    "            channel_name = info.get(\"title\", \"Channel\")\n",
    "        \n",
    "        # Remove common suffixes\n",
    "        for suffix in [\" - Videos\", \" - YouTube\"]:\n",
    "            if channel_name.endswith(suffix):\n",
    "                channel_name = channel_name[:-len(suffix)].strip()\n",
    "        \n",
    "        # Extract video IDs and URLs\n",
    "        if 'entries' in info and info['entries']:\n",
    "            for entry in info['entries']:\n",
    "                if entry is not None:\n",
    "                    video_id = entry.get('id') or entry.get('url', '').split('/')[-1]\n",
    "                    if video_id and video_id not in video_ids:\n",
    "                        video_ids.append(video_id)\n",
    "                        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                        video_urls.append(video_url)\n",
    "        elif 'id' in info:\n",
    "            video_id = info['id']\n",
    "            video_ids.append(video_id)\n",
    "            video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "            video_urls.append(video_url)\n",
    "        \n",
    "        # If no videos found in entries, try to get from playlist\n",
    "        if not video_ids and 'webpage_url' in info:\n",
    "            print(\"No videos found in entries, trying playlist extraction...\")\n",
    "            try:\n",
    "                playlist_info = ydl.extract_info(info['webpage_url'], download=False)\n",
    "                if 'entries' in playlist_info:\n",
    "                    for entry in playlist_info['entries']:\n",
    "                        if entry and 'id' in entry:\n",
    "                            video_id = entry['id']\n",
    "                            video_ids.append(video_id)\n",
    "                            video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                            video_urls.append(video_url)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if not video_ids:\n",
    "        raise ValueError(\"No videos found for this channel\")\n",
    "    \n",
    "    return channel_name, video_ids, video_urls\n",
    "\n",
    "def sanitize_filename(filename, max_length=100):\n",
    "    \"\"\"\n",
    "    Sanitiza um nome de arquivo removendo caracteres inválidos e limitando o tamanho.\n",
    "    \"\"\"\n",
    "    # Remove caracteres inválidos para nomes de arquivo\n",
    "    invalid_chars = '<>:\"/\\\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        filename = filename.replace(char, '')\n",
    "    \n",
    "    # Remove espaços extras e substitui por underscore\n",
    "    filename = '_'.join(filename.split())\n",
    "    \n",
    "    # Limita o tamanho\n",
    "    if len(filename) > max_length:\n",
    "        filename = filename[:max_length]\n",
    "    \n",
    "    return filename.strip()\n",
    "\n",
    "def get_video_title(video_id):\n",
    "    \"\"\"\n",
    "    Obtém o título do vídeo usando yt-dlp.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ydl_opts = {\n",
    "            'quiet': True,\n",
    "            'no_warnings': True,\n",
    "            'skip_download': True,\n",
    "        }\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(f\"https://www.youtube.com/watch?v={video_id}\", download=False)\n",
    "            return info.get('title', '')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def download_transcript_from_tactiq(video_url, lang_code=\"pt\"):\n",
    "    \"\"\"\n",
    "    Downloads transcript from Tactiq API for a given YouTube video URL.\n",
    "    Returns the transcript text as a string, or None if failed.\n",
    "    \"\"\"\n",
    "    url = \"https://tactiq-apps-prod.tactiq.io/transcript\"\n",
    "    \n",
    "    headers = {\n",
    "        'sec-ch-ua-platform': '\"macOS\"',\n",
    "        'Referer': 'https://tactiq.io/',\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"142\", \"Google Chrome\";v=\"142\", \"Not_A Brand\";v=\"99\"',\n",
    "        'content-type': 'application/json',\n",
    "        'sec-ch-ua-mobile': '?0'\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"videoUrl\": video_url,\n",
    "        \"langCode\": lang_code\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the response\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract transcript text from response\n",
    "        # The structure may vary, so we'll handle different possible formats\n",
    "        if isinstance(data, dict):\n",
    "            # Try common fields where transcript might be stored\n",
    "            transcript_text = data.get('transcript') or data.get('text') or data.get('content') or data.get('data')\n",
    "            if transcript_text:\n",
    "                # If it's a string, return it directly\n",
    "                if isinstance(transcript_text, str):\n",
    "                    return transcript_text\n",
    "                # If it's a list, join the items\n",
    "                if isinstance(transcript_text, list):\n",
    "                    return \"\\n\".join(str(item) for item in transcript_text)\n",
    "            \n",
    "            # If transcript is in segments/items\n",
    "            if 'segments' in data:\n",
    "                text_lines = []\n",
    "                for segment in data['segments']:\n",
    "                    if isinstance(segment, dict):\n",
    "                        text = segment.get('text') or segment.get('content') or segment.get('transcript')\n",
    "                        if text:\n",
    "                            text_lines.append(str(text))\n",
    "                    elif isinstance(segment, str):\n",
    "                        text_lines.append(segment)\n",
    "                if text_lines:\n",
    "                    return \"\\n\".join(text_lines)\n",
    "            \n",
    "            # If transcript is in items array\n",
    "            if 'items' in data:\n",
    "                text_lines = []\n",
    "                for item in data['items']:\n",
    "                    if isinstance(item, dict):\n",
    "                        text = item.get('text') or item.get('content') or item.get('transcript')\n",
    "                        if text:\n",
    "                            text_lines.append(str(text))\n",
    "                    elif isinstance(item, str):\n",
    "                        text_lines.append(item)\n",
    "                if text_lines:\n",
    "                    return \"\\n\".join(text_lines)\n",
    "            \n",
    "            # Try to find any string value in the dict\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, str) and len(value) > 50:  # Likely a transcript if long string\n",
    "                    return value\n",
    "                elif isinstance(value, list) and len(value) > 0:\n",
    "                    # Check if list contains text\n",
    "                    text_lines = []\n",
    "                    for item in value:\n",
    "                        if isinstance(item, dict):\n",
    "                            text = item.get('text') or item.get('content') or item.get('transcript')\n",
    "                            if text:\n",
    "                                text_lines.append(str(text))\n",
    "                        elif isinstance(item, str):\n",
    "                            text_lines.append(item)\n",
    "                    if text_lines:\n",
    "                        return \"\\n\".join(text_lines)\n",
    "        elif isinstance(data, str):\n",
    "            return data\n",
    "        elif isinstance(data, list):\n",
    "            # If response is a list, try to extract text from items\n",
    "            text_lines = []\n",
    "            for item in data:\n",
    "                if isinstance(item, dict):\n",
    "                    text = item.get('text') or item.get('content') or item.get('transcript')\n",
    "                    if text:\n",
    "                        text_lines.append(str(text))\n",
    "                elif isinstance(item, str):\n",
    "                    text_lines.append(item)\n",
    "            if text_lines:\n",
    "                return \"\\n\".join(text_lines)\n",
    "        \n",
    "        # If we can't parse it, return the JSON as string (for debugging)\n",
    "        print(f\"Warning: Unexpected response format. Returning JSON string.\")\n",
    "        return str(data)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching transcript from Tactiq: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing transcript response: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_transcripts(video_ids, video_urls, start_index=0, output_dir=\"transcripts\", lang_code=\"pt\"):\n",
    "    \"\"\"\n",
    "    For each video (starting from start_index), this function:\n",
    "      - Uses Tactiq API to fetch the transcript\n",
    "      - Saves the transcript as a plain text (.txt) file.\n",
    "      \n",
    "    After processing all videos, a combined file \"all_transcripts.txt\" is created in the output directory.\n",
    "    A progress counter is printed to show how many transcripts have been downloaded.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    combined_transcripts = []  # To accumulate all transcript texts.\n",
    "    total_videos = len(video_ids) - start_index\n",
    "    downloaded_count = 0\n",
    "    \n",
    "    # Delay between requests to avoid rate limiting (in seconds)\n",
    "    delay_between_requests = 1.5  # 1.5 seconds between requests\n",
    "    delay_on_error = 5.0  # 5 seconds when encountering errors\n",
    "    \n",
    "    for idx, (video_id, video_url) in enumerate(zip(video_ids[start_index:], video_urls[start_index:]), start=start_index+1):\n",
    "        # Check if old format file exists first (for backward compatibility)\n",
    "        old_format_file = os.path.join(output_dir, f\"{video_id}.txt\")\n",
    "        \n",
    "        if os.path.exists(old_format_file):\n",
    "            print(f\"[{downloaded_count}/{total_videos}] Skipping video {idx} ({video_id}): already downloaded (old format)\")\n",
    "            # Read existing file and add to combined list\n",
    "            try:\n",
    "                with open(old_format_file, 'r', encoding='utf-8') as f:\n",
    "                    existing_text = f.read()\n",
    "                combined_transcripts.append(f\"==== Video ID: {video_id} | Título: (arquivo antigo) ====\\n{existing_text}\\n\")\n",
    "                downloaded_count += 1\n",
    "            except:\n",
    "                pass\n",
    "            continue\n",
    "        \n",
    "        # Get video title (with small delay to avoid rate limiting)\n",
    "        time.sleep(0.5)  # Small delay before getting title\n",
    "        video_title = get_video_title(video_id)\n",
    "        if video_title:\n",
    "            sanitized_title = sanitize_filename(video_title)\n",
    "            filename = f\"{sanitized_title}_{video_id}.txt\"\n",
    "        else:\n",
    "            filename = f\"{video_id}.txt\"\n",
    "        \n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Check if transcript file already exists (allows resuming)\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"[{downloaded_count}/{total_videos}] Skipping video {idx} ({video_id}): already downloaded\")\n",
    "            # Read existing file and add to combined list\n",
    "            try:\n",
    "                with open(output_file, 'r', encoding='utf-8') as f:\n",
    "                    existing_text = f.read()\n",
    "                title_display = video_title if video_title else \"Sem título\"\n",
    "                combined_transcripts.append(f\"==== Video ID: {video_id} | Título: {title_display} (Language: {lang_code}) ====\\n{existing_text}\\n\")\n",
    "                downloaded_count += 1\n",
    "            except:\n",
    "                pass\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Add delay to avoid rate limiting\n",
    "            time.sleep(delay_between_requests)\n",
    "            \n",
    "            # Download transcript using Tactiq API\n",
    "            transcript_text = download_transcript_from_tactiq(video_url, lang_code=lang_code)\n",
    "            \n",
    "            if transcript_text:\n",
    "                # Save individual transcript file.\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(transcript_text)\n",
    "\n",
    "                # Append to the combined transcripts list (with a header).\n",
    "                title_display = video_title if video_title else \"Sem título\"\n",
    "                combined_transcripts.append(f\"==== Video ID: {video_id} | Título: {title_display} | Language: {lang_code} ====\\n{transcript_text}\\n\")\n",
    "                downloaded_count += 1\n",
    "                title_short = video_title[:50] + \"...\" if video_title and len(video_title) > 50 else (video_title or \"Sem título\")\n",
    "                print(f\"[{downloaded_count}/{total_videos}] ✓ Downloaded: {title_short} ({video_id}) - Language: {lang_code}\")\n",
    "            else:\n",
    "                raise Exception(\"No transcript returned from API\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            # Shorten error messages\n",
    "            if \"blocking requests\" in error_msg or \"IP\" in error_msg:\n",
    "                error_msg = \"IP bloqueado - aguardando antes de continuar...\"\n",
    "                time.sleep(delay_on_error)\n",
    "            elif \"No transcript\" in error_msg or \"transcript\" in error_msg.lower():\n",
    "                error_msg = \"Transcript não disponível\"\n",
    "            else:\n",
    "                # Truncate very long error messages\n",
    "                if len(error_msg) > 150:\n",
    "                    error_msg = error_msg[:150] + \"...\"\n",
    "            \n",
    "            print(f\"[{downloaded_count}/{total_videos}] ✗ Erro no vídeo {idx} ({video_id}): {error_msg}\")\n",
    "\n",
    "    # Write all transcripts into one combined file.\n",
    "    combined_file = os.path.join(output_dir, \"all_transcripts.txt\")\n",
    "    with open(combined_file, 'w', encoding='utf-8') as cf:\n",
    "        cf.write(\"\\n\\n\".join(combined_transcripts))\n",
    "    print(f\"Combined transcript file created: {combined_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Configure as variáveis antes de executar\n",
    "# ============================================================================\n",
    "# ⚠️ OBRIGATÓRIO: Cole a URL do canal do YouTube abaixo\n",
    "# \n",
    "# COMO FAZER:\n",
    "# 1. Vá até o canal no YouTube\n",
    "# 2. Copie a URL completa da barra de endereços\n",
    "# 3. Cole aqui entre as aspas \"\"\n",
    "#\n",
    "# Exemplos de URLs válidas:\n",
    "#   input_url = \"https://www.youtube.com/@nome_do_canal\"\n",
    "#   input_url = \"https://www.youtube.com/channel/UCxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "#   input_url = \"https://www.youtube.com/user/nome_usuario\"\n",
    "#   input_url = \"https://www.youtube.com/c/nome_do_canal\"\n",
    "#\n",
    "# ⬇️ COLE SUA URL AQUI ⬇️\n",
    "input_url = \"https://www.youtube.com/@samuelmeller\"  # ⬅️ COLE A URL DO CANAL AQUI (substitua as aspas vazias \"\")\n",
    "\n",
    "# Optional: Starting video index (1-based, so 1 = first video, 2 = second video, etc.)\n",
    "# Set to None to start from the beginning, or enter a number like 1, 2, 3, etc.\n",
    "start_index_input = None  # Set to None to start from beginning, or enter a number like 1, 2, 3, etc.\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if not input_url:\n",
    "    raise ValueError(\n",
    "        \"❌ ERRO: Você precisa definir a URL do canal!\\n\\n\"\n",
    "        \"COMO FAZER:\\n\"\n",
    "        \"1. Encontre a linha que diz: input_url = \\\"\\\"\\n\"\n",
    "        \"2. Cole a URL do canal entre as aspas\\n\"\n",
    "        \"3. Exemplo: input_url = \\\"https://www.youtube.com/@nome_do_canal\\\"\\n\\n\"\n",
    "        \"Procure a seção CONFIGURATION acima e substitua as aspas vazias pela URL do canal.\"\n",
    "    )\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Extracting video IDs, URLs and channel name...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Processing channel URL: {input_url}\")\n",
    "channel_name, video_ids, video_urls = get_video_ids_and_channel_name(input_url)\n",
    "if not video_ids or not video_urls:\n",
    "    raise ValueError(\"No videos found for this channel.\")\n",
    "\n",
    "print(f\"\\nChannel Name: {channel_name}\")\n",
    "print(f\"Found {len(video_ids)} videos.\")\n",
    "print(f\"All video links collected successfully!\\n\")\n",
    "\n",
    "# Display first few video URLs as confirmation\n",
    "print(\"Sample video URLs (first 5):\")\n",
    "for i, url in enumerate(video_urls[:5], 1):\n",
    "    print(f\"  {i}. {url}\")\n",
    "if len(video_urls) > 5:\n",
    "    print(f\"  ... and {len(video_urls) - 5} more videos\\n\")\n",
    "\n",
    "# Handle starting index\n",
    "if start_index_input is None:\n",
    "    start_index = 0\n",
    "else:\n",
    "    try:\n",
    "        start_index = int(start_index_input) - 1\n",
    "        if start_index < 0 or start_index >= len(video_ids):\n",
    "            print(\"Invalid starting index. Starting from the first video.\")\n",
    "            start_index = 0\n",
    "    except (ValueError, TypeError):\n",
    "        print(\"Invalid input. Starting from the first video.\")\n",
    "        start_index = 0\n",
    "\n",
    "# Create the output folder as transcripts/{channel_name}\n",
    "output_folder = os.path.join(\"transcripts\", channel_name)\n",
    "print(f\"Saving transcripts to folder: {output_folder}\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Save all video URLs to a file\n",
    "urls_file = os.path.join(output_folder, \"all_video_urls.txt\")\n",
    "print(f\"\\nSaving all video URLs to: {urls_file}\")\n",
    "with open(urls_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"Channel: {channel_name}\\n\")\n",
    "    f.write(f\"Total videos: {len(video_urls)}\\n\")\n",
    "    f.write(f\"Channel URL: {input_url}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    for i, url in enumerate(video_urls, 1):\n",
    "        f.write(f\"{i}. {url}\\n\")\n",
    "print(f\"✓ All {len(video_urls)} URLs saved to {urls_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"STEP 2: Downloading transcripts using Tactiq API...\")\n",
    "print(f\"Starting from video {start_index + 1}...\")\n",
    "print(\"=\" * 60)\n",
    "download_transcripts(video_ids, video_urls, start_index=start_index, output_dir=output_folder, lang_code=\"pt\")\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
